\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{bm}
\usepackage[noend]{algpseudocode}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2017}{1-48}{4/00}{10/00}{Taylor G. Smith and Jason M. White}

% Short headings should be running head and authors last names

\ShortHeadings{Correcting Class Imbalance with Variational Autoencoders}{Smith and White}
\firstpageno{1}

\begin{document}

\title{On the Synthetic Generation of Minority Class Observations Using Variational Auto-Encoders}

\author{\name Taylor G.\ Smith \email taylor.smith@alkaline-ml.com
       \AND
       \name Jason M.\ White \email jason.m.white5@gmail.com}

\editor{Michael Bernico}
\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This document describes a methodology by which to remedy imbalanced datasets for the purpose of classification. A dataset may be considered imbalanced if its classification labels are disproportionately represented across classes. In many real-world domains, class imbalance---or the presence of extremely rare events which may be costly-to-misclassify---is quite prevalent. Commonly cited problem domains include fraud detection, medical imaging, anomaly detection and countless more.  Our procedure furthers the Synthetic Minority Over-samping Technique \citep{chawla2002smote} by leveraging generative variational auto-encoders.
\end{abstract}

\begin{keywords}
  classification, class-imbalance, cost-sensitive learning, SMOTE, variational auto-encoders, skewed distributions
\end{keywords}

\section{Introduction}

This paper focuses on inducing classifiers given a training set, $X \in \mathbb{R}^{m \times n}$, with a corresponding set of class labels, $y \in \{0, 1\}$. In particular, this study explores a technique for rectifying imbalanced datasets.

A dataset may be considered imbalanced if its classification labels are disproportionately represented across classes. While class imbalance manifests itself in varying degrees, often times one class---the majority class---is highly represented, while one or more minority classes are present at a much smaller ratio. This greatly complicates the classification task since conventional, cost-insensitive evaluation metrics (such as accuracy) will offer misleadingly optimistic scores on an otherwise poor classifier.

While class imbalance is a frequent challenge in many applied machine learning domains, what makes it especially perilous is the frequent tangible cost with which misclassification of rare events is typically associated. Model selection tasks such as grid searches can be especially confounded by this issue, since the metric a grid search is intended to optimize may lie in discord with the underlying cost sensitivity of the imbalanced classification task itself. Especially susceptible to imbalanced datasets are fraud detection applications, or diagnostic classification tasks in medical datasets, where high risk examples tend to constitute the minority class \citep{rahman2013addressing}.

The machine learning community has attempted to address the problem of class imbalance in several ways: weighting---such as the development of cost-sensitive classification metrics like the ROC convex hull \citep{provost2001robust}---and data preparation tasks that either resample or manipulate the input data before building a model. Our approach can be considered one of the latter, and seeks to synthetically augment the training set irrespective of the selected scoring metric. While significant research exists to propose that simply over-sampling or under-sampling the minority class samples is not sufficient (Japkowicz, 2000), our technique can be considered a distinct form of combining the over-sampling approach with a set of generative models, and can be most closely compared to SMOTE \citep{chawla2002smote}.

Section 2 presents previous work to which our approach may be compared. Section 3 introduces generative models, and more specifically, variational auto-encoders. Section 4 outlines the details of our technique. Section 5 details the performance of our technique compared with other common class imbalance solutions. \\

\section{Previous Work}
\subsection{Over-sampling minority class samples}

Research on the efficacy of resampling as a preprocessing technique for imbalanced datasets has been extensively conducted. One common approach for resampling a training set is over-sampling the minority class examples with replacement. Japkowicz (2000) proposed several variants of over-sampling, with varying degrees of success. Her random resampling approach drew minority samples with replacement until the minority class was represented at the same magnitude as the majority class. She also proposed ``focused resampling," which only drew minority samples from along the decision boundary between the majority and minority classes. 

Of note is the fact that she observed no clear advantage between generalized over-sampling and her ``focused" over-sampling \citep{japkowicz2000learning}. Chawla, Bowyer and Hall (2002) hypothesized that this was due to the lack of decision region generalization and showed that over-sampling minority observations na\"ively could result in a reduced decision region by potentially duplicating observations that lay near a decision boundary \citep{chawla2002smote}.


\subsection{Under-sampling majority class samples}

In conjunction with her research on over-sampling methods, Japkowicz (2000) showed that under-sampling the majority class can be more effective---especially in tasks where $c > 2$---presenting the added benefit of reducing the training set size, and consequently the complexity of the training task \citep{japkowicz2000learning}. (CITE SOMETHING THAT SHOWS THE REDUCTION IN TRAINING DATA CAN BE BAD, THOUGH)

Kubat and Matwin (1997) proposed a more sophisticated form of under-sampling by which they segmented \citep{kubat1997addressing}

Chawla, Bowyer and Hall proposed the generation of synthetic minority class observations to imbalanced datasets, rather than sampling the minority class with replacement.  They implemented this approach in the Synthetic Minority Over-sampling Technique (SMOTE) algorithm \citep{chawla2002smote}. SMOTE uses k-nearest neighbors to find minority class observations most similar to each minority class observation, then generates a new observation by taking the difference between the original observation and the neighbor, multipying the difference with a random number between 0 to 1, and adding the number to the original observation. The application of SMOTE has been shown to effectively aid in generalizing the decision region for the minority class(es) by synthetically-generating minority class observations.  

--SMRT can be considered a distinct form of over-sampling the minority class, as it builds on the SMOTE paradigm by using variational autoencoders in place of k nearest neighbors to generate synthetic data. 

\section{Variational Auto-Encoders}

The advent of generative algorithms is that they do not simply learn the classification boundaries between classes, as do discriminative algorithms, but model the (often intractable) distribution by which the data was generated. As a result, generative models allow us to create new, synthetic examples after learning the underlying distribution. 

An auto-encoder, as originally introduced by Hinton and the PDP group \citep{rumelhart1985learning}, is a special case of a multilayer perceptron (MLP) that estimates its own input vector as its output. The implications of such a model offer the ability to effectively reconstruct inputs with minimal error, while identifying high-reconstruction-error examples as anomalous, or non-conformant to the underlying learned distribution. The general architecture of an auto-encoder is dual: the encoding task either compresses or expands the input signal into the hidden layer space, while the decode task projects the hidden layer-transformed values back to the input space.


A variational auto-encoder is a generative auto-encoder; it learns latent vectors, which typically approximate a unit Gaussian posterior, $\mathbf{z}$, with a diagonal covariance structure \citep{kingma2013auto}:
\begin{equation}
    \log q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)}) = \log \mathcal{N}(\mathbf{z}; \bm{\mu}^{(i)}, \bm{\sigma}^{2(i)}\mathbf{I})
\end{equation}
where the mean and standard deviation of $\mathbf{z}$ (the approximate posterior), $\bm{\mu}^{(i)}$ and $\bm{\sigma}^{(i)}$, are the results of the auto-encoder's encode task \citep{kingma2013auto}.

It achieves this by minimizing a dual loss-function: the combination of the reconstructive loss---the MSE of the input vector and the reconstructed vector---and the latent loss---the Kullback-Leibler (KL) divergence---which measures how well the latent vectors approximate the unit Gaussian:
\begin{equation}
    D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)})\parallel p_{\theta}(\mathbf{z}))
\end{equation}
Finally, synthetic examples are generated by sampling the posterior:
\begin{equation}
    \mathbf{z}^{(i,l)} \sim q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)})
\end{equation}
and finally decoding $\mathbf{z}^{(i,l)}$ with either a Bernoulli or Gaussian multilayer perceptron \citep{kingma2013auto}.


\section{SMRT}

We present a generative over-sampling approach similar to SMOTE, by which the observations in each minority class are used to fit a variational auto-encoder, and synthetic examples are generated until the class is represented at the user-specified ratio. \\

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{algorithm}
\caption{SMRT}\label{smrt}
  \begin{algorithmic}[1]
    \Procedure{Balance}{$X, y, ratio$}\Comment{Balance $X, y$ subject to $ratio$}
      \State $labels \gets \text{distinct } \textit{y}$
      \State $majority \gets argmax(count(y))$
      \State $nlabels \gets \text{length of } labels$
      \State $nreq \gets int(ratio \times \text{number of majority samples in } \textit{y})$
      \BState \emph{loop}:
      \For{i := 1 to \textit{nlabels}}
        \State $label \gets labels[i]$
        \If {$label \neq majority$}\Comment{Skip the majority class}
          \State $Xsub \gets X \text{where } y = label$
          \State $p \gets nreq - \text{length of } Xsub$\Comment{\textit{n} needed for this class}
          \State \text{Fit a variational auto-encoder with $Xsub$, call it $vae$}
          \State \text{Generate $p$ synthetic examples from $vae$, update $X, y$}
        \EndIf
      \EndFor
      \Return{$X, y$}\Comment{Optionally shuffle}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\section{SMRT Performance}


% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
  N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in

\newpage
\bibliography{references}

\end{document}
