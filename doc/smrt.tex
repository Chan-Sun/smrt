\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2017}{1-48}{4/00}{10/00}{Taylor G. Smith and Jason M. White}

% Short headings should be running head and authors last names

\ShortHeadings{Correcting Class Imbalance with Variational Autoencoders}{Smith and White}
\firstpageno{1}

\begin{document}

\title{On the Synthetic Augmentation of Minority Class Observations Using Variational Autoencoders}

\author{\name Taylor G.\ Smith \email taylor.smith@alkaline-ml.com \\
       \AND
       \name Jason M.\ White \email jason.m.white5@gmail.com}

\editor{TODO}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This document describes a methodology by which to remedy imbalanced datasets for the purpose of classification. A dataset may be considered imbalanced if its classification labels are disproportionately represented across classes. In many real-world domains, class imbalance---or the presence of extremely rare events which may be costly-to-misclassify---is quite prevalent. Such commonly cited domains include fraud detection, medical imaging, anomaly detection and countless more.  Our procedure furthers the Synthetic Minority Over-samping Technique (Chawla, Bowyer, Hall \& Kegelmeyer, 2002) by leveraging generative variational autoencoders.
\end{abstract}

\begin{keywords}
  classification, class-imbalance, cost-sensitive learning, SMOTE, variational autoencoders, skewed distributions
\end{keywords}

\section{Introduction}

Class imbalance is a frequent challenge in many applied machine learning domains. In extreme cases of disproportionately-represented classes, cost-insensitive evaluation metrics are inappropriate (CITATION NEEDED). The machine learning community has attempted to address the problem of class imbalance in two separate ways (Chawla, Bowyer, Hall \& Kegelmeyer, 2002): the development of cost-sensitive classification metrics, such as AUC (CITATION NEEDED) and the ROC convex hull (Provost \& Fawcett, 2001); and data preparation tasks that re-sample the input data before building a model. Our technique can be considered a distinct form of over-sampling the minority class, and can be most closely compared to SMOTE (Chawla, Bowyer, Hall \& Kegelmeyer, 2002).\\
        Section 2 presents previous work to which our approach may be compared. Section 3 introduces generative models, and more specifically, variational auto-encoders. Section 4 outlines the details of our technique. Section 5 details the performance of our technique compared with other commonly class imbalance solutions. \\

\section{Previous Work}

Over-sampling minority samples with replacement has been shown not to significantly improve classifier performance (Japkowicz, 2000).  Chawla, Bowyer and Hall (2002) interpretted this phenomenon as being due to the lack of decision region generalization. They hypothesized over-sampling actually minimized the decision space as minority samples were replicated. Furthermore, they showed with SMOTE that synthetically-generated minority class observations effectively aided in generalizing the decision region for the minority class(es).

\section{Generative Models}

TODO: lots of equations will go in here...

\section{SMRT}

We present a generative over-sampling approach similar to SMOTE, by which the observations in each minority class are used to fit a variational auto-encoder, and synthetic examples are generated until the class is represented at the user-specified ratio. \\

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{algorithm}
\caption{SMRT}\label{smrt}
  \begin{algorithmic}[1]
    \Procedure{Balance}{$X, y, n$}\Comment{Balance $X, y$}
      \State $labels \gets \text{distinct } \textit{y}$
      \State $majority \gets argmax(labels)$
      \State $nlabels \gets \text{length of } labels$
      \State $nreq \gets int(n \times \text{number of majority samples in } \textit{y})$
      \BState \emph{loop}:
      \For{i := 1 to \textit{nlabels}}
        \State $label \gets labels[i]$
        \If {$label \neq majority$}\Comment{Skip the majority class}
          \State $Xsub \gets X \text{where } y = label$
          \State $p \gets nreq - \text{length of } Xsub$
          \State \text{Fit a Variational Autoencoder with $Xsub$, call it $vae$}
          \State \text{Generate $p$ synthetic examples from $vae$, update $X, y$}
        \EndIf
      \EndFor
      \Return{$X, y$}\Comment{Optionally shuffle}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\section{Computation}


% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{sample}

\end{document}