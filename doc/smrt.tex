\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2017}{1-48}{4/00}{10/00}{Taylor G. Smith and Jason M. White}

% Short headings should be running head and authors last names

\ShortHeadings{Correcting Class Imbalance with Variational Autoencoders}{Smith and White}
\firstpageno{1}

\begin{document}

\title{On the Synthetic Augmentation of Minority Class Observations Using Variational Autoencoders}

\author{\name Taylor G.\ Smith \email taylor.smith@alkaline-ml.com \\
       \AND
       \name Jason M.\ White \email jason.m.white5@gmail.com}

\editor{TODO}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This document describes a methodology by which to remedy imbalanced datasets for the purpose of classification. A dataset may be considered imbalanced if its classification labels are disproportionately represented across classes. In many real-world domains, class imbalance---or the presence of extremely rare events which may be costly-to-misclassify---is quite prevalent. Such commonly cited domains include fraud detection, medical imaging, anomaly detection and countless more.  Our procedure furthers the Synthetic Minority Over-samping Technique (Chawla, Bowyer, Hall \& Kegelmeyer, 2002) by leveraging generative variational autoencoders.
\end{abstract}

\begin{keywords}
  classification, class-imbalance, cost-sensitive learning, SMOTE, variational autoencoders, skewed distributions
\end{keywords}

\section{Introduction}

Class imbalance is a frequent challenge in many applied machine learning domains. In extreme cases of disproportionately-represented classes, cost-insensitive evaluation metrics are inappropriate (CITATION NEEDED). The machine learning community has attempted to address the problem of class imbalance in two separate ways (Chawla, Bowyer, Hall \& Kegelmeyer, 2002): weighting and resampling. Weighting involves assigning a cost to examples of each class in proportion to their misclassification cost. Resampling typically involves either oversampling the minority class, or undersampling the minority class before building a model. Our technique can be considered a distinct form of over-sampling the minority class, and can be most closely compared to SMOTE (Chawla, Bowyer, Hall \& Kegelmeyer, 2002).

Section 2 presents previous work to which our approach may be compared. Section 3 introduces generative models, and more specifically, variational auto-encoders. Section 4 outlines the details of our technique. Section 5 details the performance of our technique compared with other commonly used class imbalance solutions. \\

\section{Previous Work}

Over-sampling minority samples with replacement has been shown to not significantly improve classifier performance (Japkowicz, 2000).  Chawla, Bowyer and Hall (2002) interpretted this phenomenon as being due to the lack of decision region generalization. Exposing the classifier to the same observation set multiple times allows it to learn those observations more effectively, but has the consequence of potentially overfitting those examples and therefore narrowing the decision boundry. Similarly, the work of Japkowicz showed that over-sampling minority observations that lie close to decision boundry displayed performance similar to naive over-sampling.

Due to this limitaiton, Cahwla, Boyer and Hall proposed adding synthetic minority class observations to imbalanced datasets instead of sampling the minority class with replacement.  They implemented this approach in the SMOTE algorithm. SMOTE uses k-nearest neighbors to find minority class observations most similar to each minority class observation, then generates a new observation by taking the difference between the original observation and the neighbor, multipying the difference with a random number between 0 to 1, and adding the number to the original observation. The application of SMOTE has been shown to effectively aid in generalizing the decision region for the minority class(es) by synthetically-generating minority class observations.  

--SMRT can be considered a distinct form of over-sampling the minority class, as it builds on the SMOTE paradigm by using variational autoencoders in place of k nearest neighbors to generate synthetic data. 

\section{Generative Models}

TODO: lots of equations will go in here...

\section{SMRT}

We present a generative over-sampling approach similar to SMOTE, by which the observations in each minority class are used to fit a variational auto-encoder, and synthetic examples are generated until the class is represented at the user-specified ratio. \\

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{algorithm}
\caption{SMRT}\label{smrt}
  \begin{algorithmic}[1]
    \Procedure{Balance}{$X, y, n$}\Comment{Balance $X, y$}
      \State $labels \gets \text{distinct } \textit{y}$
      \State $majority \gets argmax(labels)$
      \State $nlabels \gets \text{length of } labels$
      \State $nreq \gets int(n \times \text{number of majority samples in } \textit{y})$
      \BState \emph{loop}:
      \For{i := 1 to \textit{nlabels}}
        \State $label \gets labels[i]$
        \If {$label \neq majority$}\Comment{Skip the majority class}
          \State $Xsub \gets X \text{where } y = label$
          \State $p \gets nreq - \text{length of } Xsub$
          \State \text{Fit a Variational Autoencoder with $Xsub$, call it $vae$}
          \State \text{Generate $p$ synthetic examples from $vae$, update $X, y$}
        \EndIf
      \EndFor
      \Return{$X, y$}\Comment{Optionally shuffle}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\section{Computation}


% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{sample}

\end{document}
